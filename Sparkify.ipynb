{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Tutorial of Customer Churn Analysis & Prediction \n",
    "## Data Visualization, EDA, and Churn Prediction Using ML Algorithms for Music Streaming Service\n",
    "\n",
    "\n",
    "<a id=\"table-of-contents\"></a>\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Data Preparation](#data)\n",
    "    * 2.1. [Load and Clean Missing Data](#clean)\n",
    "    * 2.2  [Build the Website-log Dataframe (df_log)](#log) \n",
    "    * 2.3. [Transform the Webstie Logs into the User-log Data by Aggregation](#user)\n",
    "3. [Exploratory Data Analysis(EDA): Churned Vs. Stayed](#eda)\n",
    "4. [Customer Churn Prediction](#prediction)\n",
    "    * 4.1. [Engineer Features](#prediction)\n",
    "    * 4.2. [Build the Model Pipeline](#pipeline)\n",
    "    * 4.3. [Select ML Algorithms](#model) \n",
    "    * 4.4. [Tunne Hyper Parameters](#tune)\n",
    "5. [Conclusion](#conclusion)\n",
    "6. [Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Predicting churn rates is very challenging so many data scientist and anlysts struggles in any customer-facing business. Since the user-interaction services like Spotify requires to communicate with customers frequently, there are large amount of logging data every day. Thus, in this project, I would like to show how to manipulate large and realistic datasets with Spark, as well as how to build the prediction model with Spark MLlib. Let's dive in! \n",
    "\n",
    "### \n",
    "<img src=\"https://images.unsplash.com/photo-1616356607338-fd87169ecf1a?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1950&q=80\">\n",
    "<span>Photo by <a href=\"https://unsplash.com/@fhavlik?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Filip Havlik</a> on <a href=\"https://unsplash.com/s/photos/music-streaming-phone?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a></span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "# 2.  Data Preparation \n",
    "\n",
    "First, you need to import lot's of Spark libralies as below, then you can start opening the instance of SparkSession to wranlge the big data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from plotly import graph_objs as go\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import pyspark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel, Estimator, Transformer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Sparkify\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clean\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "## 2.1. Load and Clean Missing Data \n",
    "\n",
    "### Load Data\n",
    "In this project, we are going to usethe website logging data collected from the virtual music streaming company, called \"Sparkify\". The original size of this dataset is 12GB, but we can start exploring data with the subset of them -`mini_sparkify_event_data.json`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df = spark.read.json(\"mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printthe basic inforamation of the dataset\n",
    "print(f\"Total records of the data: {df.count()}\")\n",
    "df.printSchema()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the unique categories of each columns to understand data structure\n",
    "for col in ['auth', 'level', 'page',  'gender','location','artist', 'song',]: \n",
    "    unique_cols = df.select(col) \\\n",
    "                    .groupBy(col).count() \\\n",
    "                    .orderBy(\"count\", ascending=False) \\\n",
    "                    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Missing Data\n",
    "Now it's time to clean up some empty and invalid data. If you run the code below, you can see that there are some users with empty string, who probably not regular users in Sparkify. So we can discard those users from our dataset. Also, you can see that there are a lot of None values in \"song\" and other columns, but we don't need to take care of them for now (We are going to transform this dataset for each user so those song and artists data are not critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the missing / invalid values in each column\n",
    "for col in df.columns: \n",
    "    empty_count = df.where(df[col] == \"\").count()\n",
    "    none_count = df.where(df[col].isNull()).count()\n",
    "    print(f\"{col}: \\t\\tempty({empty_count}), \\tnone({none_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the missing values for each column\n",
    "pd_isnull = df.toPandas().isnull().replace({True:1, False:0}) \n",
    "\n",
    "trace = go.Heatmap(\n",
    "            x=pd_isnull.columns.tolist(),\n",
    "            y=pd_isnull.index.tolist(),\n",
    "            z=pd_isnull.values.tolist(), \n",
    "            xgap=0.5,\n",
    "            colorscale=[[0,'black'], [1,'whitesmoke']], \n",
    "            showscale=False,\n",
    "        )\n",
    "    \n",
    "layout = dict(title = dict(\n",
    "                text='Missing Data HeatMap',\n",
    "                x=0.5, \n",
    "                y=0.9,\n",
    "                xanchor='center',\n",
    "                yanchor='top', \n",
    "                font_size=25, \n",
    "            ),\n",
    "            plot_bgcolor = 'darkgrey', \n",
    "            paper_bgcolor = 'rgb(243,243,243)', \n",
    "            font = dict(\n",
    "                family='Times New Roman', \n",
    "                size=15,\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                title='columns', \n",
    "                ticks='outside', \n",
    "                tickangle=-45, \n",
    "                side='top'\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title='index', \n",
    "                showticklabels=False\n",
    "            ),\n",
    "            margin=dict(t=200,b=10),\n",
    "        )\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the empty userID since invalid userID generates 8346 \"none\"s in \"firstName\", \"gender\", \"lastName\", \"location\", \"registration\", and \"userAgent\"\n",
    "# for \"nones\" in the \"artist\" and \"song\" columns, we can leave them because it is an optional information when a user plays some music. \n",
    "df_clean = df.where(df.userId != \"\")\n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "## 2.2. Build the Webstie-Log Dataset (df_log) \n",
    "\n",
    "### Understand the Website Log data\n",
    "I wrote down some descriptions of the Sparkify user transaction data in the below table. Also I highlighted some important variables that used in this analysis. \n",
    "\n",
    "\n",
    "| Column | Type |  Description| Comment|\n",
    "| --- | --- | --- | --- |\n",
    "|**ts** |(long) | the timestamp of user log |  |\n",
    "| sessionId |(long) | an identifier for the current session |  |\n",
    "| auth |(string)| the authentification log |'LoggedIn','LoggedOut', 'Guest', 'Cancelled' |\n",
    "| itemInSession |(long)| the number of items in a single session |  |\n",
    "| method |(string)| HTTP request method |'put', 'get'|\n",
    "| status |(long)| http status |307:Temporary Redirect, 404:Not Found, 200:OK)|\n",
    "| userAgent |(string)| the name of HTTP user agent  | |\n",
    "| **userId** | (string) | the user Id | |\n",
    "| **gender** | (string) | the user gender | 'F', 'M' |\n",
    "| **location** | (string) | the user location  | |\n",
    "| firstName | (string) | the user first name  | |\n",
    "| lastName | (string) | the user last  | |\n",
    "| **registration** | (long) | the timestamp when user registered | |\n",
    "| **level** | (string) | the level of user subscription | 'free','paid' |\n",
    "| **page** | (string) | the page name visited by users | 'Home', 'Login', 'LogOut','Settings', 'Save Settings','about', 'NextSong', <br> 'Thumbs Up', 'Thumbs Down', 'Add to Playlist','Add Friend', 'Roll Advert',<br> 'Upgrade', 'Downgrade', 'help','Submit Downgrade', 'Cancel','Cancellation Confrimation' |\n",
    "| **artist** | (string) | the name of the artist played by users | |\n",
    "| **song** | (string) | the name of songs played by users | |\n",
    "| length | (double) | the length of a song in seconds | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn\n",
    "\n",
    "The customer churn is defined when existing customers cancel the subscription. In this project, I define the churn status as 1 when a user visit the page `Cancellation Confirmation` only. Since this datset shows only two months, if someone submit the downgrade before Oct, the level of the person is 'free', but not churned! Thus we need to analyze churn rate for both free and paid users. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the subscription level is changed for single user when the user hits the `submit Downgrade` page\n",
    "df_clean.where((df_clean.userId == '131') & (df_clean.page != 'NextSong')).select('page', 'level').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see who cancelled \n",
    "df_clean.select(['userId', 'page','level']).where(df_clean.page == 'Cancellation Confirmation').show(5)\n",
    "\n",
    "# show the total cancels \n",
    "churned_users = df_clean.where(df_clean.page == \"Cancellation Confirmation\").select(\"userId\").dropDuplicates().count()\n",
    "total_users = df_clean.select(\"userId\").dropDuplicates().count()\n",
    "\n",
    "print(f\"The count of churned users (1): {churned_users}\")\n",
    "print(f\"The count of Not-Churned users (0): {total_users - churned_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Website-Log Dataframe (df_log) by Adding More Columns\n",
    "From the cleaned dataset (df_clean), we can build the website-log dataframe (df_log) with our target variable `churn` and some additional columns as below.  \n",
    "- `churn`(int): The status which indicates if a user is churned(1) or not(0)  \n",
    "- `ds` (datetime): DateStamp(ds) converted from the timestamp data \"ts\"\n",
    "- `dsRestration` (datetime): DateStamp(ds) converted from the timestamp data \"registration\"\n",
    "- `locCity` (str): the name of city from \"location\" data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add datetime columns from timestamp columns\n",
    "to_datestr = f.udf(lambda t: datetime.datetime.fromtimestamp(t / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "is_cancelled = f.udf(lambda page: 1 if page == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "w = Window.partitionBy('userId')\n",
    "df_log = df_clean.withColumn('churnEvent', is_cancelled(f.col('page'))) \\\n",
    "                 .withColumn('churn', f.max(f.col('churnEvent')).over(w)) \\\n",
    "                 .withColumn('ds', f.to_date(to_datestr(f.col('ts')))) \\\n",
    "                 .withColumn('dsRegistration', f.to_date(to_datestr(f.col('registration')))) \\\n",
    "                 .withColumn(\"locCities\", f.split(df.location, ',').getItem(0)) \\\n",
    "                 .withColumn(\"locCity\", f.split(f.col('locCities'), '-').getItem(0)) \\\n",
    "                 .drop('churnEvent','locCities') # intermediate columns\n",
    "\n",
    "# show result\n",
    "df_log.dropDuplicates(subset=['userId']).select(['userId', 'churn', 'ds', 'dsRegistration', 'locCity']).show(3)\n",
    "df_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the df_log\n",
    "\n",
    "print(f'The shape of the raw data: {df.toPandas().shape}')\n",
    "print(f'The shape of the clean data: {df_clean.toPandas().shape}')\n",
    "print(f'The shape of the log data: {df_log.toPandas().shape}')\n",
    "\n",
    "total_users = df_log.dropDuplicates([\"userId\"]).count()\n",
    "churned_users = df_log.where(f.col('churn')==1).dropDuplicates(['userId']).count()\n",
    "stayed_users = df_log.where(f.col('churn')==0).dropDuplicates(['userId']).count()\n",
    "print(f'The number of users (unique userId): {total_users}')  \n",
    "print(f\"The count of churned users (1): {churned_users}\")\n",
    "print(f\"The count of Not-Churned users (0): {stayed_users}\")\n",
    "\n",
    "log_period = df_log.select(f.min('ds'), f.max('ds')).first()\n",
    "print(f'The logging period: {log_period[0]} - {log_period[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "## 2.3. Transform the Webstie Logs into the User-log Data by Aggregation\n",
    "\n",
    "In order to predict churn status for users, the website-log data needs to be transformed for each user. First, we need to discard some columns that are not related to customer churn events such as session logs and user names. Then, we can transform data based on userId and there are two types of data: user information and user activities. User information columns in our data are churn, gender, level, and locCity, which must be the same for each user.\n",
    "\n",
    "For user activity data, we need to aggregate the logging data to create some meaningful features. I listed the new columns that I added to the user-log dataset below. \n",
    "- lifeTime (long): the user lifetime is how long a user has been alive on the website, and the number indicates days from the registration date to the last active log date\n",
    "- playTime (double): the song playtime is the average time(sec) of the total songs played by a user while the user visits next songpage\n",
    "- numSongs (long): the total number of song names for each user \n",
    "- numArtists (long): the total number of artist names for each user\n",
    "- numPage_* (long): the total number of page visits for each page and each user. Note that Cancellation and Conform cancellation pages are not considered for the feature group because those are used for generating churn labels. Also, Login and Register have zero counts for all users in our dataset, so they are automatically dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. add lifeTime \n",
    "w =  Window.partitionBy('userId')\n",
    "df_life = df_log.select('userId', 'ds', 'dsRegistration') \\\n",
    "               .withColumn('dsLastLog', f.max('ds').over(w)) \\\n",
    "               .withColumn('lifeTime', f.datediff(f.col('dsLastLog'), f.col('dsRegistration'))) \\\n",
    "               .dropDuplicates(subset=['userId']) \\\n",
    "               .drop('ds','registration', 'dsRegistration', 'dsLastLog')\n",
    "df_life.show(5)\n",
    "df_life.count() # make sure total 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. add playTime\n",
    "# user defined function to set flag if a user visits 'home' or not \n",
    "is_homevisit = f.udf(lambda ishome : int(ishome == 'Home'), IntegerType())\n",
    "# define windows to partition \n",
    "w1 = Window.partitionBy('userID') \\\n",
    "           .orderBy(f.desc('ts')) \\\n",
    "           .rangeBetween(Window.unboundedPreceding, 0)\n",
    "w2 = Window.partitionBy(['userId', 'songCnt'])\n",
    "w3 = Window.partitionBy('userId')\n",
    "# add playTime column (# 1000: msec -> sec )\n",
    "df_play = df_log.select('userId', 'page', 'ts') \\\n",
    "               .where((f.col('page') == 'NextSong') | (f.col('page') == 'Home')) \\\n",
    "               .withColumn('homevisit', is_homevisit(f.col('page'))) \\\n",
    "               .withColumn('songCnt', f.sum('homevisit').over(w1)) \\\n",
    "               .withColumn('songTime', f.max('ts').over(w2) - f.min('ts').over(w2)) \\\n",
    "               .dropDuplicates(subset=['userId', 'songCnt']) \\\n",
    "               .withColumn('playTime', f.avg('songTime').over(w3) / 1000) \\\n",
    "               .dropDuplicates(subset=['userId']) \\\n",
    "               .drop('page','ts','homevisit', 'songCnt', 'songTime')\n",
    "\n",
    "df_play.show(3)\n",
    "df_play.count() # make sure total 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. num of songs \n",
    "is_valid = f.udf(lambda x: 1 if x else 0, IntegerType()) # null \n",
    "w = Window.partitionBy('userId')\n",
    "df_song = df_log.select('userId', 'song') \\\n",
    "                .dropDuplicates(['userId','song']) \\\n",
    "                .withColumn('songValid', is_valid(f.col('song'))) \\\n",
    "                .withColumn('numSongs', f.sum('songValid').over(w)) \\\n",
    "                .dropDuplicates(['userID']) \\\n",
    "                .drop('song', 'songValid')\n",
    "\n",
    "df_song.show(3)\n",
    "df_song.count() # make sure total 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. num of artists \n",
    "is_valid = f.udf(lambda x: 1 if x else 0, IntegerType()) # null \n",
    "w = Window.partitionBy('userId')\n",
    "df_arts = df_log.select('userId', 'artist') \\\n",
    "                .dropDuplicates(['userId','artist']) \\\n",
    "                .withColumn('artistValid', is_valid(f.col('artist'))) \\\n",
    "                .withColumn('numArtists', f.sum('artistValid').over(w)) \\\n",
    "                .dropDuplicates(['userID']) \\\n",
    "                .drop('artist', 'artistValid')\n",
    "\n",
    "df_arts.show(3)\n",
    "df_arts.count() # make sure total 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. pages  \n",
    "print(df.where(df.page == 'Login').count(), df_clean.where(df_clean.page =='Login').count())\n",
    "print(df.where(df.page == 'Register').count(), df_clean.where(df_clean.page =='Register').count())\n",
    "\n",
    "w = Window.partitionBy('userId','page')\n",
    "df_page = df_log.select('userId','page')\\\n",
    "    .withColumn('pageVisits',f.count('page').over(w))\\\n",
    "    .groupBy('userId') \\\n",
    "    .pivot('page') \\\n",
    "    .max('pageVisits')\\\n",
    "    .na.fill(0) \\\n",
    "    .drop('page', 'Cancel', 'Cancellation Confirmation') # don't include features related to churn\n",
    "\n",
    "# rename the columns with prefix 'numPage'\n",
    "page_names = df_page.columns\n",
    "page_names.remove('userId')\n",
    "df_page = df_page.select(['userId']+[f.col(c).alias('numPage_' + c.replace(\" \", \"\")) for c in page_names])\n",
    "\n",
    "print(df_page.take(1))\n",
    "print(df_page.toPandas().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict churn for each user so that we need to extract features based on 'userId'\n",
    "# user data \n",
    "df_user = df_log.select('userId', 'churn', 'gender', 'level', 'locCity') \\\n",
    "                .dropDuplicates(subset=['userId']) \\\n",
    "                .join(df_life, on='userId') \\\n",
    "                .join(df_play, on='userId') \\\n",
    "                .join(df_song, on='userId') \\\n",
    "                .join(df_arts, on='userId') \\\n",
    "                .join(df_page, on='userId') \\\n",
    "        \n",
    "# show result\n",
    "print(f'The shape of the User-Log data(df_user): {df_user.toPandas().shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.select('userId', 'churn', 'gender', 'level', 'locCity').show(3)\n",
    "df_user.select('lifeTime', 'PlayTime', 'numSongs', 'numArtists', 'numPage_About').show(3)\n",
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eda\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "# 3. Exploratory Data Analysis (EDA): Churned Vs. Stayed\n",
    "\n",
    "For the visualization, I used the Plotly libray to make interactive plots. The main code is long so I hide the cells below. If you want to check, please expand the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# this cell contains helper functions to draw plots using Plotly\n",
    "\n",
    "# define the global color map for churned users and stayed user\n",
    "fig_colors = {'churned':'rgba(171, 50, 96, 0.6)', 'stayed':'rgba(12, 50, 196, 0.6)'} # churn, stayed\n",
    "\n",
    "\n",
    "def draw_barplot(x, y_churned, y_stayed, topic=''): \n",
    "    \"\"\" draw a stacked bar plot for two user groupbs (churned and stayed) \n",
    "        using plotly library \n",
    "    [Args] \n",
    "        x (list): the data list for x axis\n",
    "        y_churned (list) : the y axis data for 'churned' trace\n",
    "        y_stayed (list): the y axis data for 'stayed' trace\n",
    "        topic (str): the str used for title \n",
    "    [Returns] \n",
    "        fig (obj): plotly go.Figure() object \n",
    "    \"\"\"\n",
    "    y_total = []\n",
    "    y_total.append(int(y_churned[0] + y_stayed[0]))\n",
    "    y_total.append(int(y_churned[1] + y_stayed[1]))\n",
    "    \n",
    "    trace1 = go.Bar(\n",
    "        name=\"Churned\", \n",
    "        x=x,  \n",
    "        y=y_churned,\n",
    "        text=[f\"Churn Rate: {100*(cnt/(total+1e-10)) :.2f}%<br>\" \\\n",
    "              for cnt,total in zip(y_churned,y_total)], \n",
    "        textposition='auto',\n",
    "        hovertext = [f\"Churn Rate: {100*(cnt/(total+1e-10)) :.2f}%<br>\" \\\n",
    "                     + f\"Churned Users: {cnt :.0f} / {total :.0f}\" \n",
    "                     for cnt,total in zip(y_churned,y_total)], \n",
    "        marker=dict(color=fig_colors['churned']), \n",
    "        opacity=0.75, \n",
    "    )\n",
    "\n",
    "    trace2 = go.Bar(\n",
    "        name=\"Stayed\", \n",
    "        x=x,  \n",
    "        y=y_stayed,\n",
    "        text=[f\"Stay Rate: {100*(cnt/(total+1e-10)) :.2f}%\" for cnt,total in zip(y_stayed,y_total)], \n",
    "        textposition='auto',\n",
    "        hovertext = [f\"Churn Rate: {100*(cnt/(total+1e-10)) :.2f}%<br>\" \\\n",
    "                     + f\"Churned Users: {cnt :.0f} / {total :.0f}\" \n",
    "                     for cnt,total in zip(y_stayed,y_total)], \n",
    "        marker=dict(color=fig_colors['stayed']), \n",
    "        opacity=0.75, \n",
    "    )\n",
    "\n",
    "    layout = dict(\n",
    "        barmode = 'stack', \n",
    "        hovermode = 'x', \n",
    "        title = dict(\n",
    "            text=f'The Churn Analysis: {topic}',\n",
    "            x=0.5, \n",
    "            y=0.95,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "            font_size=25, \n",
    "        ),\n",
    "        yaxis_title = 'Number of Users',\n",
    "        legend = dict(\n",
    "            orientation='h', \n",
    "            x=0.5, \n",
    "            y=1.15,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "        ), \n",
    "        plot_bgcolor = 'rgb(243,243,243)', \n",
    "        paper_bgcolor = 'rgb(243,243,243)', \n",
    "        font = dict(\n",
    "            family='Times New Roman', \n",
    "            size=15,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace2, trace1], layout=layout)\n",
    "\n",
    "    for item, total in zip(x, y_total): \n",
    "        fig.add_annotation(\n",
    "            x=item, y=total, yshift=25,showarrow=False,\n",
    "            text=f\"Total in {item}: {total}\",\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def draw_timeplot(x, y_churned, y_stayed, topic=''): \n",
    "    \"\"\" draw a stacked bar plot for two user groupbs (churned and stayed) \n",
    "        with additional line traces  \n",
    "    [Args] \n",
    "        x (list): the data list for x axis\n",
    "        y_churned (list) : the y axis data for 'churned' trace\n",
    "        y_stayed (list): the y axis data for 'stayed' trace\n",
    "        topic (str): the str used for title \n",
    "    [Returns] \n",
    "        fig (obj): plotly go.Figure() object \n",
    "    \"\"\"\n",
    "    offset = 15\n",
    "\n",
    "    trace1 = go.Scatter(\n",
    "        name=\"Churned\", \n",
    "        x=x,  \n",
    "        y=np.array(y_churned) + np.array(y_stayed) - offset,\n",
    "        marker=dict(color=fig_colors['churned'], symbol='square', size=15), \n",
    "        line=dict(color=fig_colors['churned'], width=2, dash='dash'), \n",
    "        showlegend=False,\n",
    "        hoverinfo='none'\n",
    "    )\n",
    "\n",
    "    trace2 = go.Scatter(\n",
    "        name=\"Stayed\", \n",
    "        x=x,  \n",
    "        y=np.array(y_stayed) - offset,\n",
    "        marker=dict(color=fig_colors['stayed'], symbol='square', size=15), \n",
    "        line=dict(color=fig_colors['stayed'], width=2, dash='dash'), \n",
    "        showlegend=False,\n",
    "        hoverinfo='none'\n",
    "    )\n",
    "    \n",
    "    fig = draw_barplot(x, y_churned, y_stayed, topic=topic)\n",
    "    # clear the text on bar plots \n",
    "    for trace in fig['data']: \n",
    "        trace['textposition'] = 'none'\n",
    "    \n",
    "    # add lineplots \n",
    "    fig.add_traces([trace1, trace2])\n",
    "    \n",
    "    # add annotations\n",
    "    for xi, yi, shift, value in zip(trace1['x'], trace1['y'], [-30, +30], y_churned): \n",
    "        fig.add_annotation(\n",
    "            x=xi, y=yi, xshift=shift,showarrow=False,\n",
    "            text=f'{value:.0f}',\n",
    "            font=dict(color=fig_colors['churned'])\n",
    "        )\n",
    "    for xi, yi, shift, value in zip(trace2['x'], trace2['y'], [-30, +30], y_stayed): \n",
    "        fig.add_annotation(\n",
    "            x=xi, y=yi, xshift=shift,showarrow=False,\n",
    "            text=f'{value:.0f}',\n",
    "            font=dict(color=fig_colors['stayed'])\n",
    "        )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def stack_bars_horizontally(figs, topics=\"\"):\n",
    "    \"\"\" draw a combined multiple bar plots \n",
    "        using plotly library \n",
    "    [Args] \n",
    "        figs (list of objs): list of go.Figure() object to combine \n",
    "        topics (str): the str used for title \n",
    "    [Returns] \n",
    "        fig (obj): plotly go.Figure() object \n",
    "    \"\"\"\n",
    "    # Combine bar plots horizontaly \n",
    "    if not topics:\n",
    "        topics = ', '.join([str(f.data[0].y0) for f in figs]) \n",
    "    \n",
    "    fig_bars = go.Figure().set_subplots(rows=1, cols=len(figs), shared_yaxes=True)\n",
    "\n",
    "    for i, fig in enumerate(figs): \n",
    "        fig_bars.add_trace(fig.data[0], row=1, col=i+1)\n",
    "        fig_bars.add_trace(fig.data[1], row=1, col=i+1)\n",
    "        fig_bars.update_xaxes(fig.layout.xaxis, row=i+1, col=1)\n",
    "        fig_bars.update_yaxes(fig.layout.yaxis, row=i+1, col=1)\n",
    "\n",
    "        if i < (len(figs)-1): \n",
    "            fig_bars.update_traces(showlegend=False)\n",
    "\n",
    "        fig_bars.add_annotation(fig.layout.annotations[0], row=1, col=i+1)\n",
    "        fig_bars.add_annotation(fig.layout.annotations[1], row=1, col=i+1)\n",
    "\n",
    "\n",
    "    fig_bars.update_layout(dict(\n",
    "        barmode = 'stack', \n",
    "        hovermode = 'x', \n",
    "        title = dict(\n",
    "            text=f'Churn Analysis: ' + topics,\n",
    "            x=0.5, \n",
    "            y=0.95,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "            font_size=25, \n",
    "        ),\n",
    "        yaxis_title = 'Number of Users',\n",
    "        legend = dict(\n",
    "            orientation='h', \n",
    "            x=0.5, \n",
    "            y=1.15,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "        ), \n",
    "        plot_bgcolor = 'rgb(243,243,243)', \n",
    "        paper_bgcolor = 'rgb(243,243,243)', \n",
    "        font = dict(\n",
    "            family='Times New Roman', \n",
    "            size=15,\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    return fig_bars\n",
    "    \n",
    "    \n",
    "def draw_geoplot(pd_city):\n",
    "    \"\"\" draw a geoplot for US city using plotly library \n",
    "    [Args] \n",
    "        pd_city (Spark Dataframe): the dataframe with the geographical information \n",
    "    [Returns] \n",
    "        fig (obj): plotly go.Figure() object \n",
    "    \"\"\"\n",
    "    limits = [(0, 0.05), (0.05,0.25),(0.25,0.50),(0.50,0.75),(0.75,.95),(0.95,1.001)]\n",
    "    scale = 150\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(limits)):\n",
    "        lim = limits[i]\n",
    "        pd_sub = pd_city.loc[(pd_city['churnRate']>=lim[0]) & (pd_city['churnRate'] <lim[1]), :]\n",
    "        data.append(go.Scattergeo(\n",
    "            locationmode = 'USA-states',\n",
    "            lon = pd_sub['lon'],\n",
    "            lat = pd_sub['lat'],\n",
    "            text = pd_sub['text'],\n",
    "            marker = dict(\n",
    "                size = pd_sub['churnRate']*scale + 10,\n",
    "                opacity = 0.5,\n",
    "                line_color='rgb(217, 217, 217)',\n",
    "                line_width= 0.5,\n",
    "                sizemode = 'area',\n",
    "            ),\n",
    "            name = f'{100*lim[0]:.0f}% - {100*lim[1]:.0f}%'\n",
    "          )\n",
    "        )\n",
    "    \n",
    "    layout = dict(\n",
    "        geo = dict(\n",
    "            scope = 'usa',\n",
    "            landcolor = 'rgb(217, 217, 217)',\n",
    "            bgcolor = 'rgb(243,243,243)',\n",
    "        ),\n",
    "        title = dict(\n",
    "            text='The Churn Analysis: Location',\n",
    "            x=0.5, \n",
    "            y=0.9,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "            font_size=25, \n",
    "        ),\n",
    "        showlegend = True,\n",
    "        legend_title='   Churn Rate',\n",
    "        plot_bgcolor = 'rgb(243,243,243)', \n",
    "        paper_bgcolor = 'rgb(243,243,243)', \n",
    "        font = dict(\n",
    "            family='Times New Roman', \n",
    "            size=15,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def draw_violinplot(x_churned, x_stayed, topic='', unit=''):\n",
    "    \"\"\" draw a horizontal violin  plot for two user groupbs (churned and stayed) \n",
    "        using plotly library \n",
    "    [Args] \n",
    "        y_churned (list) : the data for 'churned' trace\n",
    "        y_stayed (list): the data for 'stayed' trace\n",
    "        topic (str): the str used for title \n",
    "        unit (str): additional unit information \n",
    "    [Returns] \n",
    "        fig (obj): plotly go.Figure() object \n",
    "    \"\"\"\n",
    "    trace1 = go.Violin(\n",
    "        x=x_churned,\n",
    "        y0=topic,\n",
    "        width=1,\n",
    "        name='Churned', scalegroup='Churned', legendgroup='Churned', \n",
    "        side='positive',\n",
    "        marker=dict(color=fig_colors['churned']),\n",
    "        pointpos=0.05,\n",
    "    )\n",
    "\n",
    "    trace2 = go.Violin(\n",
    "        x=x_stayed,\n",
    "        y0=topic,\n",
    "        width=1,\n",
    "        name='Stayed',scalegroup='Stayed', legendgroup='Stayed', \n",
    "        side='negative',\n",
    "        marker=dict(color=fig_colors['stayed']),\n",
    "        pointpos=-0.05,\n",
    "    )\n",
    "\n",
    "    #update characteristics shared by all traces \n",
    "    trace_all = dict(\n",
    "        box_visible=True,\n",
    "        box_width=0.8,\n",
    "        meanline_visible=True,\n",
    "        meanline_width=3,\n",
    "        opacity=0.75,\n",
    "        jitter=0.02,\n",
    "        scalemode='width',\n",
    "        orientation='h',\n",
    "#         points='all',\n",
    "    )\n",
    "\n",
    "    layout = dict(\n",
    "        violinmode='overlay',\n",
    "        violingroupgap=0, violingap=0,\n",
    "        title = dict(\n",
    "            text=f'The Churn Analysis: {topic}',\n",
    "            x=0.5, \n",
    "            y=0.9,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "            font_size=25, \n",
    "        ),\n",
    "        yaxis=dict(title=f'{topic}', showticklabels=False),\n",
    "        xaxis=dict(ticksuffix=f' {unit}',rangemode='tozero'),\n",
    "        showlegend = True,\n",
    "        plot_bgcolor = 'rgb(243,243,243)', \n",
    "        paper_bgcolor = 'rgb(243,243,243)', \n",
    "        font = dict(\n",
    "            family='Times New Roman', \n",
    "            size=15,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "    fig.update_traces(trace_all)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Combine violin plots vertically  \n",
    "def stack_violins_vertically(figs, topics=\"\"):\n",
    "    \"\"\" draw a vertically stacked violin plots \n",
    "    [Args] \n",
    "        figs (list of objs): list of go.Figure() object to combine \n",
    "        topics (str): the str used for title \n",
    "    [Returns] \n",
    "        fig (obj): plotly go.Figure() object \n",
    "    \"\"\"\n",
    "    if not topics:\n",
    "        topcis = ', '.join([str(f.data[0].y0) for f in figs]) \n",
    "    \n",
    "    fig_violins = go.Figure().set_subplots(rows=len(figs), cols=1, vertical_spacing=0.05)\n",
    "    for i, fig in enumerate(figs): \n",
    "        fig_violins.add_trace(fig.data[0], row=i+1, col=1)\n",
    "        fig_violins.add_trace(fig.data[1], row=i+1, col=1)\n",
    "\n",
    "        fig_violins.update_xaxes(fig.layout.xaxis, row=i+1, col=1)\n",
    "        fig_violins.update_yaxes(fig.layout.yaxis, row=i+1, col=1)\n",
    "        fig_violins.update_yaxes(title=\"\", showticklabels=True, ticks='outside', row=i+1, col=1)\n",
    "        if i < (len(figs)-1): \n",
    "            fig_violins.update_traces(showlegend=False)    \n",
    "\n",
    "    # fig_violins.update_traces(box_visible=True,points='all', jitter=0.1, scalemode='count')\n",
    "    fig_violins.update_layout(dict(\n",
    "        violinmode='overlay',\n",
    "        title = dict(\n",
    "            text=f'The Churn Analysis: {topics}',\n",
    "            x=0.5, \n",
    "            y=0.97,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "            font_size=25, \n",
    "        ),\n",
    "        showlegend = True,\n",
    "#             legend=dict(orientation='h', x=0.42, xanchor='center', y=1.06, yanchor='top'),\n",
    "        legend = dict(\n",
    "            orientation='h', \n",
    "            x=0.42, \n",
    "            y=1.05,\n",
    "            xanchor='center',\n",
    "            yanchor='top', \n",
    "        ), \n",
    "        plot_bgcolor = 'rgb(248,248,248)', \n",
    "        paper_bgcolor = 'rgb(243,243,243)', \n",
    "        font = dict(\n",
    "            family='Times New Roman', \n",
    "            size=15,\n",
    "        ), \n",
    "        height=1000,\n",
    "#         margin_t=100, \n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return fig_violins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Churn Analysis: Time \n",
    "\n",
    "# churn rate per month \n",
    "df_oct_log = df_log.where((df_log['ds'] >= datetime.date(2018, 10, 1)) & (df_log['ds'] < datetime.date(2018, 11, 1)))\n",
    "df_nov_log = df_log.where((df_log['ds'] >= datetime.date(2018, 11, 1)) & (df_log['ds'] < datetime.date(2018, 12, 1)))\n",
    "\n",
    "# total count of users across oct and nov \n",
    "y_usercount = []\n",
    "y_usercount.append(df_oct_log.dropDuplicates([\"userId\"]).count())\n",
    "y_usercount.append(df_nov_log.dropDuplicates([\"userId\"]).count())\n",
    "\n",
    "y_churnrate = [] \n",
    "y_churnrate.append(df_oct_log.dropDuplicates(['userId']).select(f.mean('churn')).collect()[0][0])\n",
    "y_churnrate.append(df_nov_log.dropDuplicates(['userId']).select(f.mean('churn')).collect()[0][0])\n",
    "\n",
    "# Plot\n",
    "x = ['October', 'November']\n",
    "y_churned = [cnt*rate for cnt,rate in zip(y_usercount, y_churnrate)]\n",
    "y_stayed = [cnt*(1-rate) for cnt,rate in zip(y_usercount, y_churnrate)]\n",
    "fig_time = draw_timeplot(x, y_churned, y_stayed, topic='Time')\n",
    "fig_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Churn Analysis: Gender\n",
    "pd_gender = df_user.groupBy(['gender', 'churn']) \\\n",
    "                   .agg(f.count('churn').alias('churnCnt')) \\\n",
    "                   .orderBy('gender') \\\n",
    "                   .toPandas()\n",
    "\n",
    "x = ['Female', 'Male'] \n",
    "y_churned = pd_gender[pd_gender.churn == 1][\"churnCnt\"].tolist() #[femaleChurned, maleChurned]\n",
    "y_stayed = pd_gender[pd_gender.churn == 0][\"churnCnt\"].tolist() #[femaleStayed, maleStayed]\n",
    "\n",
    "fig_gender = draw_barplot(x, y_churned, y_stayed, topic='Gender')\n",
    "# fig_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Churn Analysis: Subscription Level\n",
    "pd_level = df_user.groupBy(['level', 'churn']) \\\n",
    "                   .agg(f.count('churn').alias('churnCnt')) \\\n",
    "                   .orderBy('level') \\\n",
    "                   .toPandas()\n",
    "\n",
    "x = ['Free', 'Paid'] \n",
    "y_churned = pd_level[pd_level.churn == 1][\"churnCnt\"].tolist()\n",
    "y_stayed = pd_level[pd_level.churn == 0][\"churnCnt\"].tolist()\n",
    "\n",
    "fig_level = draw_barplot(x, y_churned, y_stayed, topic='Subscription Level')\n",
    "# fig_level.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the integratedbar plots \n",
    "figs = [fig_gender, fig_level]\n",
    "fig_bars = stack_bars_horizontally(figs, topics='Gender And Subscription Level')\n",
    "fig_bars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Churn Analysis: City Location  \n",
    "\n",
    "# select the data of city and churn\n",
    "pd_city = df_log[['locCity', 'churn']]\\\n",
    "            .groupBy('locCity')\\\n",
    "            .agg(f.count('churn').alias('userCount'), f.sum('churn').alias('churnCount'))\\\n",
    "            .toPandas()\n",
    "\n",
    "# pull the us city location(long,lan) data from the external source\n",
    "pd_usloc = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_us_cities.csv')\n",
    "pd_usloc = pd_usloc.loc[:,['name', 'lat', 'lon']]\n",
    "pd_usloc['name'] = pd_usloc['name'].str.strip()\n",
    "pd_usloc = pd_usloc.rename(columns={'name':'locCity'})\n",
    "pd_usloc = pd_usloc.drop_duplicates('locCity')\n",
    "\n",
    "# display the differnece\n",
    "print(list(set(pd_city.locCity.unique()) - set(pd_usloc.locCity.unique())))\n",
    "\n",
    "#replace the different names\n",
    "pd_usloc = pd_usloc.replace({'Winston-Salem':'Winston', 'New London':'London'})\n",
    "# add new data\n",
    "pd_missing =pd.DataFrame([['Alexandria', 38.8048, -77.0469], \n",
    "                          ['Birmingham', 33.5186, -86.8104], \n",
    "                          ['Anchorage', 61.2181, -149.9003],\n",
    "                          ['Morgantown', 39.6295, -79.9559],\n",
    "                          ['Hagerstown', 39.6418, -77.7200],\n",
    "                          ['North Wilkesboro', 36.1585, -81.1476],\n",
    "                          ['Santa Maria', 34.9530, -120.4357],\n",
    "                          ['Allentown', 40.602, -75.4714],\n",
    "                          ['Manchester', 42.9956, -71.4548],\n",
    "                          ['Fairbanks', 64.8378, -147.7164],\n",
    "                          ['Flint', 43.0125, -83.6875]], columns=pd_usloc.columns)\n",
    "pd_usloc = pd.concat([pd_usloc, pd_missing], axis=0, ignore_index=True)\n",
    "\n",
    "# display the differnece\n",
    "print(list(set(pd_city.locCity.unique()) - set(pd_usloc.locCity.unique())))\n",
    "\n",
    "# join two dataframe \n",
    "pd_city = pd.merge(pd_city, pd_usloc, how='left', on='locCity')\n",
    "\n",
    "# add more detail info for the figure \n",
    "pd_city['churnRate'] = pd_city['churnCount'] / pd_city['userCount']\n",
    "pd_city['text'] = pd_city['locCity'].apply(lambda x: f'In {x} city <br>')\n",
    "pd_city['text'] += pd_city['churnCount'].apply(lambda x: f' Churn Count: {x} <br>')\n",
    "pd_city['text'] += pd_city['userCount'].apply(lambda x: f' User Count: {x} <br>')\n",
    "pd_city['text'] += pd_city['churnRate'].apply(lambda x: f' Churn Rate: {x*100:.2f} %')\n",
    "\n",
    "# sort dataframe along churnCount\n",
    "# pd_city = pd_city.sort_values('churnCount')\n",
    "print(pd_city.loc[pd_city.churnRate > 0, :].describe())\n",
    "\n",
    "#plot\n",
    "fig_city = draw_geoplot(pd_city)\n",
    "fig_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Churn Analysis: User Lifetime\n",
    "# :User lifetime is define as the duration from the registration to the last activity log \n",
    "pd_lifetime = df_user.select('userid','lifeTime','churn') \\\n",
    "                 .toPandas()\n",
    "x_churned = pd_lifetime['lifeTime'][pd_lifetime['churn'] == 1]\n",
    "x_stayed = pd_lifetime['lifeTime'][pd_lifetime['churn'] == 0]\n",
    "\n",
    "fig_life = draw_violinplot(x_churned, x_stayed, topic='User Life Time', unit='days')\n",
    "# fig_life.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Churn Analysis: Song Playing Time\n",
    "pd_playtime = df_user.select('playTime','churn').toPandas()\n",
    "x_churned = pd_playtime['playTime'][pd_playtime['churn'] == 1] / 3600 # sec->hours\n",
    "x_stayed = pd_playtime['playTime'][pd_playtime['churn'] == 0] / 3600\n",
    "\n",
    "fig_play = draw_violinplot(x_churned, x_stayed, topic='Average Song Playing Time', unit='hrs')\n",
    "# fig_play.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Churn Analysis : number of songs\n",
    "pd_numsongs = df_user.select('numSongs','churn').toPandas()\n",
    "x_churned = pd_numsongs['numSongs'][pd_numsongs['churn'] == 1] \n",
    "x_stayed = pd_numsongs['numSongs'][pd_numsongs['churn'] == 0] \n",
    "\n",
    "fig_song = draw_violinplot(x_churned, x_stayed, topic='Number of Songs Played', unit='songs')\n",
    "# fig_song.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Churn Analysis : number of Artists\n",
    "pd_numarts = df_user.select('numArtists','churn').toPandas()\n",
    "x_churned = pd_numarts['numArtists'][pd_numarts['churn'] == 1] \n",
    "x_stayed = pd_numarts['numArtists'][pd_numarts['churn'] == 0] \n",
    "\n",
    "fig_arts = draw_violinplot(x_churned, x_stayed, topic='Number of Artists', unit='artists')\n",
    "# fig_arts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the combined violin plots for numerical variables \n",
    "fig_list = [fig_life, fig_play, fig_song, fig_arts]\n",
    "fig_violins1 = stack_violins_vertically(fig_list, topics='Numerical Features')\n",
    "fig_violins1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Churn Analysis : Pages\n",
    "pd_pages = df_user.select(['churn'] + [c for c in df_user.columns if c.lower().find('page') > -1]).toPandas()\n",
    "y_churned = pd_pages.loc[pd_pages['churn'] == 1, pd_pages.columns.drop('churn')]\n",
    "y_stayed = pd_pages.loc[pd_pages['churn'] == 0, pd_pages.columns.drop('churn')]\n",
    "\n",
    "fig_pages = {}\n",
    "for i, col in enumerate(y_churned.columns): \n",
    "    fig_pages[col] = draw_violinplot(y_churned[col], y_stayed[col], \n",
    "                                     topic=f'Page Visits<br>({col.split(\"_\")[1]})', \n",
    "                                     unit='visits')\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_names = ['numPage_About', 'numPage_Error', 'numPage_SubmitDowngrade', 'numPage_RollAdvert', 'numPage_ThumbsDown', 'numPage_Upgrade'] \n",
    "fig_list = [fig_pages[name] for name in page_names]\n",
    "fig_violins2 = stack_violins_vertically(fig_list, topics='Numerical Features - PageVisits')\n",
    "fig_violins2.show()\n",
    "\n",
    "fig_list = list(fig_pages.values())[:6]\n",
    "fig_violins_all = stack_violins_vertically(fig_list, topics='Numerical Features - PageVisits')\n",
    "fig_violins_all.show()\n",
    "\n",
    "fig_list = list(fig_pages.values())[6:12]\n",
    "fig_violins_all = stack_violins_vertically(fig_list, topics='Numerical Features - PageVisits')\n",
    "fig_violins_all.show()\n",
    "\n",
    "fig_list = list(fig_pages.values())[12:]\n",
    "fig_violins_all = stack_violins_vertically(fig_list, topics='Numerical Features - PageVisits')\n",
    "fig_violins_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prediction\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "# 4. Customer Churn Prediction\n",
    "\n",
    "Now it's time to build a model and predict if a user is going to churn or not. With this information, the business model can be personalized, such as providing special promotions. For this, we need Feature Engineer first, and then find the proper model, and finally, we can deploy this model to the real business in the future.  \n",
    "\n",
    "## 4.1.Engineer Features\n",
    "From the visualization, we can finally select our features for the prediction model by modifying the user-log dataset as below.\n",
    "- `LocCity` will be dropped from the feature set since it has many extreme values\n",
    "- `numSongs` and `numArts` are highly correlated (0.99) so numSongs will be chosen only for the feature set\n",
    "- `numPage_SubmitDowngrade` will be converted to the categorical feature `page_SubmitDowngrade` having only two values: ‘visited’ or ‘none’\n",
    "- The features related to the `page` column have many correlations each other, so I selected only 7 features:`numPage_About`, `numPage_Error`, `numPage_RollAdvert`, `numPage_SaveSettings`, `numPage_SubmitDowngrade`, `numPage_ThumbsDown`, `numPage_Upgrade`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the feature dataframe\n",
    "def show_corr(df): \n",
    "    plt.figure(figsize = (16,16))\n",
    "    sns.heatmap(df.drop('userId').toPandas().corr(), annot=True, fmt='.2g'\\\n",
    "                ,vmin=-1,vmax=1,center=0,cmap='coolwarm',square=True)\n",
    "    plt.savefig(\"corr_heatmap.png\")\n",
    "# Feature Selection from correlation graph \n",
    "show_corr(df_user) \n",
    "non_features = ['userId', 'churn']\n",
    "category_cols = ['gender', 'level', 'page_SubmitDowngrade'] # drop LocCity\n",
    "numeric_cols = ['lifeTime', 'playTime', 'numSongs', 'numPage_About', 'numPage_Error', 'numPage_RollAdvert', 'numPage_SaveSettings', \n",
    "                 'numPage_SubmitUpgrade', 'numPage_ThumbsDown', 'numPage_Upgrade'] \n",
    "# show_corr(df_user.select(non_features + numeric_cols)) \n",
    "\n",
    "is_visit = f.udf(lambda x: 'visited' if x > 0 else 'none', StringType())\n",
    "df_feat = df_user.withColumn('page_SubmitDowngrade', is_visit(f.col('numPage_SubmitDowngrade'))) \\\n",
    "                 .drop('numPage_SubmitDowngrade') \\\n",
    "                 .withColumnRenamed('churn', 'label') \\\n",
    "                 .select(['userId', 'label'] + category_cols + numeric_cols)\n",
    "\n",
    "\n",
    "df_feat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "## 4.2. Build the Model Pipeline\n",
    "Let’s start to build the pipeline for the prediction model using ML libraries in Spark. For better cross-validation, I combined all feature transformations and an estimator into one pipeline and feed it into the `CrossValidator`. There are three main parts of the pipeline.\n",
    "\n",
    "1. **Feature Transformation**: category variables will be transformed to one-hot encoded vectors by `StringIndexer` and `OneHotEncoder`. Then, the categorical vector and numerical variables will be assembled into a dense feature vector using `VectorAssembler`.\n",
    "\n",
    "2. **Feature Importance Selection**: I built a custom `FeatureSelector` class to extract only important features using a Tree-based estimator. This step is optional so that I didn’t use it for Logistic Regression or LinearSVC models.\n",
    "\n",
    "3. **Estimator**: The final step is using ML algorithms to estimate the churn label for each user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class FeatureSelector(Estimator): \n",
    "    \"\"\" Custom FeatureSelector class inherited from the Estimator class defined in PySpark\n",
    "        to add the ML pipeline later \n",
    "    \"\"\"\n",
    "    def __init__(self, estimator=None, threshold = 0.01, outputCol='features'): \n",
    "        \"\"\" initialize the FeatureSelector variables to use later\n",
    "        [Args]\n",
    "            estimator (object) : tree-based estimator with featureImportances attributes \n",
    "            threshold (float): the threshold value to select features \n",
    "            outputCol (str): the name of output column having the selected features \n",
    "        [Returns]\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(FeatureSelector, self).__init__()\n",
    "        self._setDefault()\n",
    "        self.estimator = estimator\n",
    "        self.threshold = threshold\n",
    "        self.outputCol = outputCol \n",
    "        \n",
    "    def _fit(self, data): \n",
    "        \"\"\" fit the data using the tree-based estimator to extract feautre importnace. \n",
    "            Then, only select the improtant features over the threshold\n",
    "        [Args]\n",
    "            data (Spark Dataframe): dataframe for feature selection\n",
    "        [Returns]\n",
    "            (Spark Dataframe): return the dataframe with the selected features \n",
    "        \"\"\"\n",
    "        model = self.estimator.fit(data)\n",
    "        pred = model.transform(data)\n",
    "        df_featImp = self.ExtractFeatureImportance(model.featureImportances, pred, self.estimator.getFeaturesCol())\n",
    "        feat_idx = [x for x in df_featImp.loc[df_featImp['score'] > self.threshold, 'idx']] \n",
    "        return VectorSlicer(inputCol = self.estimator.getFeaturesCol(), \n",
    "                            outputCol = self.outputCol, \n",
    "                            indices=feat_idx)\n",
    "        \n",
    "    def ExtractFeatureImportance(self, featureImp, dataset, featuresCol):\n",
    "        list_extract = []\n",
    "        for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "            list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "            \n",
    "        varlist = pd.DataFrame(list_extract)\n",
    "        varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "        \n",
    "        return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer: \n",
    "    \"\"\"Custom trainer class to run the ML pipeline Using Cross Validation \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, category_cols, numeric_cols): \n",
    "        \"\"\" initialize the dataset infomation and split the data for the pipeline \n",
    "        [Args]\n",
    "            data (Spark Dataframe): the dataset used for ML prediction learning and testing \n",
    "            cateogry_cols (list of str): the list of category column names in the dataset \n",
    "            numeric_cols (list of str): the list of numerical column names in the dataset \n",
    "        [Returns]\n",
    "            None\n",
    "        \"\"\"\n",
    "        # split the data into train and test \n",
    "        self.train_data, self.test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "        \n",
    "        # initialize internal variables \n",
    "        self.cat_cols = category_cols\n",
    "        self.num_cols = numeric_cols\n",
    "        \n",
    "        \n",
    "    def run(self, est, params=ParamGridBuilder().build(), feature_selector_on=False, restart=False, prefix='base'):\n",
    "        \"\"\" run the ML pipline using cross validation \n",
    "        [Args] \n",
    "            est (object): estimator object defined in the PySpark ml library  \n",
    "            params (object): ParamGridBuilder object for cross validation \n",
    "            feature_selector_on (bool): featureSelction step in the pipeline will be turned on if it is True\n",
    "            restart (bool): Trainer will generate a new model if it is True, otherwise, it will load the pre-trained model(./input/models)\n",
    "            prefix (str): it is the prefix string for the file name which is used to save the trained model\n",
    "        [Returns] \n",
    "            None (just print some status and result directly)\n",
    "        \"\"\"\n",
    "        print('--------------------')\n",
    "        # construct feature transform pipeline\n",
    "        string_indexers = [StringIndexer(inputCol = c, outputCol = c + 'Idx', handleInvalid = 'keep') for c in self.cat_cols]\n",
    "        onehot_indexers = [OneHotEncoder(inputCols = [c + 'Idx'], outputCols = [c + 'Vec']) for c in self.cat_cols]\n",
    "        assembler = VectorAssembler(inputCols = self.num_cols + [c + 'Vec' for c in self.cat_cols], outputCol = \"features\")\n",
    "#         label_indexer = StringIndexer(inputCol = 'churn', outputCol = 'label', handleInvalid = 'keep') # it makes class 3 not 2\n",
    "       \n",
    "        stages = string_indexers + onehot_indexers + [assembler] #, label_indexer]\n",
    "        \n",
    "        # make the use ofFeatureSelector optional depending on estimator\n",
    "        if est and feature_selector_on: # est should be tree-based algorithms \n",
    "            selector = FeatureSelector(estimator = est.copy(), threshold=0.01, outputCol='features_subset')\n",
    "            stages.append(selector)\n",
    "            # update the estimator feature columns \n",
    "            est.setFeaturesCol('features_subset')\n",
    "        \n",
    "        # define the crossValidator \n",
    "        pipe = Pipeline(stages = stages + [est])\n",
    "        evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "        crossval = CrossValidator(estimator=pipe,\n",
    "                                  evaluator=evaluator, \n",
    "                                  estimatorParamMaps=params,\n",
    "                                  numFolds=3,\n",
    "                                  seed=42)\n",
    "        \n",
    "        # dfeine the model path to save or load \n",
    "        model_name = str(est).split(\"_\")[0]\n",
    "        file_name = f'{prefix}_pipeline_{model_name}.pth'\n",
    "        model_path = os.path.join('models', file_name)\n",
    "        \n",
    "        # train and select the best model\n",
    "        if not restart and os.path.exists(model_path):           \n",
    "            best_pipe = PipelineModel.load(model_path)\n",
    "            print(f\"load best pipeline from {model_path}\")\n",
    "        else: \n",
    "            print(\"running cross-validation ...\")\n",
    "            cv_model = crossval.fit(self.train_data)\n",
    "            best_pipe = cv_model.bestModel\n",
    "            best_pipe.write().overwrite().save(model_path) # save to kaggle output\n",
    "            print(f'cv valid_f1: {cv_model.avgMetrics}')\n",
    "\n",
    "        train_pred = best_pipe.transform(self.train_data) \n",
    "        test_pred = best_pipe.transform(self.test_data)\n",
    "        \n",
    "        # show the final results \n",
    "        print(f'best estimator: {best_pipe.stages[-1]}')\n",
    "        print(f'parameters: {[(p.name, v) for p, v in best_pipe.stages[-1].extractParamMap().items()]}')\n",
    "        print(f'train_f1: {evaluator.evaluate(train_pred):.4f}, test_f1: {evaluator.evaluate(test_pred):.4f}')\n",
    "        if feature_selector_on: \n",
    "            feat_importance = selector.ExtractFeatureImportance(\n",
    "                                best_pipe.stages[-1].featureImportances, \n",
    "                                train_pred, \n",
    "                                est.getFeaturesCol())\n",
    "            print(feat_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Selection \n",
    "trainer = Trainer(df_feat, category_cols, numeric_cols)\n",
    "\n",
    "trainer.run(LogisticRegression(), feature_selector_on=False)\n",
    "trainer.run(LinearSVC(), feature_selector_on=False)\n",
    "trainer.run(DecisionTreeClassifier(), feature_selector_on=True)\n",
    "trainer.run(RandomForestClassifier(), feature_selector_on=True)\n",
    "trainer.run(GBTClassifier(), feature_selector_on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tune\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "## 4.4. Tune Hyper Parameters\n",
    "\n",
    "Finally, I ran the cross-validation to tune the hyper-parameters of the `RandomForestClassifier`. **Since our dataset is very small, you can observe an almost perfect train score, pointing that model is overfitted**. Thus, I selected some cross-validation parameter maps to make the model less complex compared to the default model that I used for the above model selection (numTrees=20). **The result shows that the model with 10 trees and 16 max bins has slightly better performance but it didn’t overcome the overfitting problem well. I assume that this problem can be solved by adding more data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hyper Parameter Tunning \n",
    "est = RandomForestClassifier()\n",
    "params=ParamGridBuilder() \\\n",
    "        .addGrid(est.numTrees, [5, 10, 20]) \\\n",
    "        .addGrid(est.maxDepth, [3, 5]) \\\n",
    "        .addGrid(est.maxBins, [16, 32]) \\\n",
    "        .build()\n",
    "\n",
    "trainer.run(est=est, params=params, feature_selector_on=True, prefix='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "# 5. [Conclusion](#conclusion)\n",
    "\n",
    "In this article, we tackled one of the most challenging and common business problems - how to predict customer churns. From the nasty and huge website logs, we extracted several meaningful features for each user, and visualize them based on two user groups (churned vs. stayed) for more analysis. Finally, we built the ML pipeline including feature transformations and estimator, which fed to the cross-validator for model selection and hyper-parameter tuning. The final model shows a pretty high testing score (f1-score: 0.73), but it also has an overfitting problem due to the limitation of the small dataset size (128MB). Since Udacity provides the full dataset (12GB) on AWS cloud, I have a plan to deploy this Spark cluster to handle the overfitting problem soon. \n",
    "\n",
    "Absolutely, there are many things we can do to improve this model without considering data size. First, most features are just aggregated regardless of the time factor. The logs are collected for 2 months so it would be better to emphasize more recent ones with different methods like a weighted sum.  Also, we can apply some strategies to handle the data imbalance (this [blog](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/) will help you get some ideas). Furthermore, we can model this problem as time series models because churn rates should be reported periodically to the business stakeholders. \n",
    "\n",
    " I hope that this project gives you a tutorial on how to deal with large data to solve a real-world problem using data science and machine learning skills. Plus, it can be good practice to play with the Spark and Plotly libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "[back to top](#table-of-contents)\n",
    "\n",
    "# 6. [Reference](#reference)\n",
    "\n",
    "- This project is the part of the [Udacity Data Scientist Nanodegree Program](https://www.udacity.com/course/data-scientist-nanodegree--nd025). The topic and dataset are given from the Udacity but the code and contents are written by myself. \n",
    "\n",
    "- To create the pipline, I got some help from this [blog](https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
